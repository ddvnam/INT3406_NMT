# Transformer NMT Configuration

# Model parameters
model:
  d_model: 512           # Model dimension
  n_layers: 6            # Number of encoder/decoder layers
  n_heads: 8             # Number of attention heads
  dropout: 0.1           # Dropout rate

# Data parameters
data:
  # Training data paths
  train_src: "D:/Data/AI/DeepLearning/nlp_final_project_2025/task1_transformer/data/raw/train.en.txt"
  train_trg: "D:/Data/AI/DeepLearning/nlp_final_project_2025/task1_transformer/data/raw/train.vi.txt"
  
  # Validation data paths
  val_src: "D:/Data/AI/DeepLearning/nlp_final_project_2025/task1_transformer/data/raw/tst2013.en.txt"
  val_trg: "D:/Data/AI/DeepLearning/nlp_final_project_2025/task1_transformer/data/raw/tst2013.vi.txt"
  
  # Language codes
  src_lang: "en"
  trg_lang: "vi"
  
  # Maximum sequence length
  max_strlen: 100
  
  # Vocabulary settings
  min_freq: 2            # Minimum frequency for vocabulary

# Training parameters
training:
  # Training hyperparameters
  batch_size: 8
  num_epochs: 1
  validate_every: 1      # Validate every N epochs
  early_stopping_patience: 5
  
  # Optimizer settings
  init_lr: 0.2           # Initial learning rate
  warmup_steps: 4000     # Warmup steps for LR schedule
  betas: [0.9, 0.98]     # Adam beta parameters
  eps: 1e-9              # Adam epsilon
  
  # Gradient clipping
  clip: 1.0
  
  # Training settings
  device: "auto"         # "auto", "cuda", or "cpu"
  seed: 42
  
  # Logging and saving
  save_dir: "checkpoints"
  log_freq: 10           # Log every N batches
  save_interval: 5       # Save checkpoint every N epochs

# Inference parameters
inference:
  max_length: 100
  beam_size: 5
  temperature: 1.0